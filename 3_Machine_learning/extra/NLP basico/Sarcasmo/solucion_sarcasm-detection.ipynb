{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb01ca96934e5c83a36a2308da9645b87a9c52a0"
   },
   "source": [
    "## <center> Assignment 4. Sarcasm detection with logistic regression\n",
    "    \n",
    "We'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n",
    "\n",
    "Sarcasm detection is easy. \n",
    "<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4"
   },
   "outputs": [],
   "source": [
    "# some necessary imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "b23e4fc7a1973d60e0c6da8bd60f3d921542a856"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train-balanced-sarcasm.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"
   },
   "source": [
    "Some comments are missing, so we drop the corresponding rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_42788/3812176965.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_df['comment'] = train_df['comment'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "# Todo lo que no sea caracter alfanumerico o espacio, sustituye por vacio\n",
    "train_df['comment'] = train_df['comment'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['label',\n",
    " 'comment',\n",
    " 'author',\n",
    " 'subreddit',\n",
    " 'score',\n",
    " 'ups',\n",
    " 'downs',\n",
    " 'date',\n",
    " 'created_utc',\n",
    " 'parent_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today but since Gr...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isnt funny none of the new york nigg...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                          NC and NH  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today but since Gr...   Creepeth   \n",
       "3      0  This meme isnt funny none of the new york nigg...  icebrotha   \n",
       "4      0                     I could use one of those tools  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0a7ed9557943806c6813ad59c3d5ebdb403ffd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1010773 entries, 0 to 1010825\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count    Dtype \n",
      "---  ------          --------------    ----- \n",
      " 0   label           1010773 non-null  int64 \n",
      " 1   comment         1010773 non-null  object\n",
      " 2   author          1010773 non-null  object\n",
      " 3   subreddit       1010773 non-null  object\n",
      " 4   score           1010773 non-null  int64 \n",
      " 5   ups             1010773 non-null  int64 \n",
      " 6   downs           1010773 non-null  int64 \n",
      " 7   date            1010773 non-null  object\n",
      " 8   created_utc     1010773 non-null  object\n",
      " 9   parent_comment  1010773 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 84.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d51637ee70dca7693737ad0da1dbb8c6ce9230b"
   },
   "source": [
    "We notice that the dataset is indeed balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "addd77c640423d30fd146c8d3a012d3c14481e11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    505405\n",
       "1    505368\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b836574e5093c5eb2e9063fefe1c8d198dcba79"
   },
   "source": [
    "We split data into training and validation parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c200add4e1dcbaa75164bbcc73b9c12ecb863c96"
   },
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = train_test_split(train_df['comment'], train_df['label'], random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"
   },
   "source": [
    "## Tasks:\n",
    "1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n",
    "2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n",
    "3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n",
    "4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n",
    "\n",
    "## Links:\n",
    "  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n",
    "  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n",
    "  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n",
    "  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.2.2-cp39-cp39-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.1/153.1 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": [
           505405,
           505368
          ],
          "colorscale": "Picnic",
          "reversescale": true
         },
         "type": "bar",
         "uid": "b5ec6128-5a78-4eda-85ab-c487c7905fb8",
         "x": [
          0,
          1
         ],
         "y": [
          505405,
          505368
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 11
        },
        "title": {
         "text": "Number of Labels"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_lbl = train_df['label'].value_counts()\n",
    "trace = go.Bar(\n",
    "    x=cnt_lbl.index,\n",
    "    y=cnt_lbl.values,\n",
    "    marker=dict(\n",
    "        color=cnt_lbl.values,\n",
    "        colorscale = 'Picnic',\n",
    "        reversescale = True\n",
    "    ),\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Number of Labels',\n",
    "    font=dict(size=11)\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"TargetCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "labels": [
          0,
          1
         ],
         "type": "pie",
         "uid": "7240b5af-5bdb-411a-976f-e480df008116",
         "values": [
          50.001830282368054,
          49.99816971763195
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 11
        },
        "height": 600,
        "title": {
         "text": "Label distribution"
        },
        "width": 600
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = (np.array(cnt_lbl.index))\n",
    "sizes = (np.array((cnt_lbl / cnt_lbl.sum())*100))\n",
    "\n",
    "trace = go.Pie(labels=labels, values=sizes)\n",
    "layout = go.Layout(\n",
    "    title='Label distribution',\n",
    "    font=dict(size=11),\n",
    "    width=600,\n",
    "    height=600,\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"usertype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_42788/3229390776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(20.0,12.0), \n\u001b[0;32m      4\u001b[0m                    title = None, title_size=40, image_color=False):\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(20.0,12.0), \n",
    "                   title = None, title_size=40, image_color=False):\n",
    "    \n",
    "    stopwords = set(STOPWORDS)\n",
    "    more_stopwords = {'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n",
    "    stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = max_words,\n",
    "                    max_font_size = max_font_size, \n",
    "                    random_state = 42,\n",
    "                    width=800, \n",
    "                    height=400,\n",
    "                    mask = mask)\n",
    "    wordcloud.generate(str(text))\n",
    "    \n",
    "    plt.figure(figsize=figure_size)\n",
    "    if image_color:\n",
    "        image_colors = ImageColorGenerator(mask);\n",
    "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
    "        plt.title(title, fontdict={'size': title_size,  \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    else:\n",
    "        plt.imshow(wordcloud);\n",
    "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    plt.axis('off');\n",
    "    plt.tight_layout()  \n",
    "    \n",
    "plot_wordcloud(train_df[\"comment\"], title=\"Word Cloud of Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from plotly import tools\n",
    "\n",
    "train1_df = train_df[train_df[\"label\"]==1]\n",
    "train0_df = train_df[train_df[\"label\"]==0]\n",
    "\n",
    "## custom function for ngram generation ##\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "## custom function for horizontal bar chart ##\n",
    "def horizontal_bar_chart(df, color):\n",
    "    trace = go.Bar(\n",
    "        y=df[\"word\"].values[::-1],\n",
    "        x=df[\"wordcount\"].values[::-1],\n",
    "        showlegend=False,\n",
    "        orientation = 'h',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        ),\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "## Get the bar chart from not sarcastic comment ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in train0_df[\"comment\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "## Get the bar chart from sarcastic comment ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in train1_df[\"comment\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words of not sarcastic comments\", \n",
    "                                          \"Frequent words of sarcastic comments\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq_dict = defaultdict(int)\n",
    "for sent in train0_df[\"comment\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n",
    "\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in train1_df[\"comment\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent bigrams of not sarcastic questions\", \n",
    "                                          \"Frequent bigrams of sarcastic questions\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq_dict = defaultdict(int)\n",
    "for sent in train0_df[\"comment\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n",
    "\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in train1_df[\"comment\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n",
    "                          subplot_titles=[\"Frequent trigrams of not sarcastic comments\", \n",
    "                                          \"Frequent trigrams of sarcastic comments\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "s = train0_df['comment'].to_json()\n",
    "ngram_counts = Counter(ngrams(s.split(), 2))\n",
    "print(ngram_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"comment\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train_df[\"num_stopwords\"] = train_df[\"comment\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "train_df[\"num_punctuations\"] =train_df['comment'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"comment\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "color = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Truncate some extreme values for better visuals ##\n",
    "train_df['num_words'].loc[train_df['num_words']>60] = 60 #truncation for better visuals\n",
    "train_df['num_chars'].loc[train_df['num_chars']>350] = 350 #truncation for better visuals\n",
    "\n",
    "f, axes = plt.subplots(2, 1, figsize=(10,20))\n",
    "sns.boxplot(x='label', y='num_words', data=train_df, ax=axes[0])\n",
    "axes[0].set_xlabel('label', fontsize=12)\n",
    "axes[0].set_title(\"Number of words in each class\", fontsize=15)\n",
    "\n",
    "sns.boxplot(x='label', y='num_chars', data=train_df, ax=axes[1])\n",
    "axes[1].set_xlabel('label', fontsize=12)\n",
    "axes[1].set_title(\"Number of characters in each class\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.loc[train_df['label'] == 1, 'comment'].str.len().apply(np.log1p).hist(label='sarcastic', alpha=.5)\n",
    "train_df.loc[train_df['label'] == 0, 'comment'].str.len().apply(np.log1p).hist(label='not sarcastic', alpha=.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizamos si en media hay subreddits más sarcasticas que otras\n",
    "\n",
    "#Ordenados por total de comentarios\n",
    "\n",
    "sub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\n",
    "sub_df.sort_values(by='sum', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordenados de subreddits más sarcásticas a menos.\n",
    "\n",
    "sub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bigrams, put a limit on maximal number of features and minimal word frequency\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\n",
    "\n",
    "# multinomial logistic regression a.k.a softmax classifier\n",
    "logit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n",
    "                           random_state=17, verbose=1)\n",
    "# sklearn's pipeline\n",
    "tfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n",
    "                                 ('logit', logit)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_logit_pipeline.fit(train_texts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "valid_pred = tfidf_logit_pipeline.predict(valid_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_valid, valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pintamos la matriz de confusión\n",
    "\n",
    "def plot_confusion_matrix(actual, predicted, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix', figsize=(7,7),\n",
    "                          cmap=plt.cm.Blues, path_to_save_fig=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(actual, predicted).T\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Predicted label')\n",
    "    plt.xlabel('True label')\n",
    "    \n",
    "    if path_to_save_fig:\n",
    "        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_valid, valid_pred, \n",
    "                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logit_pipeline.named_steps['tf_idf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos observar los pesos de cada palabra para los comentarios sarcásticos\n",
    "\n",
    "import eli5\n",
    "eli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n",
    "                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
